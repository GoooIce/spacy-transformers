# Stubs for pytorch_transformers.tokenization_bert (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .tokenization_utils import PreTrainedTokenizer, clean_up_tokenization
from typing import Any, Optional

logger: Any
VOCAB_FILES_NAMES: Any
PRETRAINED_VOCAB_FILES_MAP: Any
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES: Any

def load_vocab(vocab_file: Any): ...
def whitespace_tokenize(text: Any): ...

class BertTokenizer(PreTrainedTokenizer):
    vocab_files_names: Any = ...
    pretrained_vocab_files_map: Any = ...
    max_model_input_sizes: Any = ...
    vocab: Any = ...
    ids_to_tokens: Any = ...
    do_basic_tokenize: Any = ...
    basic_tokenizer: Any = ...
    wordpiece_tokenizer: Any = ...
    def __init__(self, vocab_file: Any, do_lower_case: bool = ..., do_basic_tokenize: bool = ..., never_split: Optional[Any] = ..., unk_token: str = ..., sep_token: str = ..., pad_token: str = ..., cls_token: str = ..., mask_token: str = ..., tokenize_chinese_chars: bool = ..., **kwargs: Any) -> None: ...
    @property
    def vocab_size(self): ...
    def convert_tokens_to_string(self, tokens: Any): ...
    def save_vocabulary(self, vocab_path: Any): ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: Any, *inputs: Any, **kwargs: Any): ...

class BasicTokenizer:
    do_lower_case: Any = ...
    never_split: Any = ...
    tokenize_chinese_chars: Any = ...
    def __init__(self, do_lower_case: bool = ..., never_split: Optional[Any] = ..., tokenize_chinese_chars: bool = ...) -> None: ...
    def tokenize(self, text: Any, never_split: Optional[Any] = ...): ...

class WordpieceTokenizer:
    vocab: Any = ...
    unk_token: Any = ...
    max_input_chars_per_word: Any = ...
    def __init__(self, vocab: Any, unk_token: Any, max_input_chars_per_word: int = ...) -> None: ...
    def tokenize(self, text: Any): ...
