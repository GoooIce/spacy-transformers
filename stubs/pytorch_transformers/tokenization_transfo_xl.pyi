# Stubs for pytorch_transformers.tokenization_transfo_xl (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .file_utils import cached_path
from .tokenization_utils import PreTrainedTokenizer, clean_up_tokenization
from typing import Any, Optional

logger: Any
VOCAB_FILES_NAMES: Any
PRETRAINED_VOCAB_FILES_MAP: Any
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES: Any
PRETRAINED_CORPUS_ARCHIVE_MAP: Any
CORPUS_NAME: str

class TransfoXLTokenizer(PreTrainedTokenizer):
    vocab_files_names: Any = ...
    pretrained_vocab_files_map: Any = ...
    max_model_input_sizes: Any = ...
    counter: Any = ...
    special: Any = ...
    min_freq: Any = ...
    max_size: Any = ...
    lower_case: Any = ...
    delimiter: Any = ...
    vocab_file: Any = ...
    never_split: Any = ...
    def __init__(self, special: Optional[Any] = ..., min_freq: int = ..., max_size: Optional[Any] = ..., lower_case: bool = ..., delimiter: Optional[Any] = ..., vocab_file: Optional[Any] = ..., pretrained_vocab_file: Optional[Any] = ..., never_split: Optional[Any] = ..., unk_token: str = ..., eos_token: str = ..., additional_special_tokens: Any = ..., **kwargs: Any) -> None: ...
    def count_file(self, path: Any, verbose: bool = ..., add_eos: bool = ...): ...
    def count_sents(self, sents: Any, verbose: bool = ...) -> None: ...
    def save_vocabulary(self, vocab_path: Any): ...
    idx2sym: Any = ...
    sym2idx: Any = ...
    def build_vocab(self) -> None: ...
    def encode_file(self, path: Any, ordered: bool = ..., verbose: bool = ..., add_eos: bool = ..., add_double_eos: bool = ...): ...
    def encode_sents(self, sents: Any, ordered: bool = ..., verbose: bool = ...): ...
    def add_special(self, sym: Any) -> None: ...
    def add_symbol(self, sym: Any) -> None: ...
    def convert_tokens_to_string(self, tokens: Any): ...
    def convert_to_tensor(self, symbols: Any): ...
    @property
    def vocab_size(self): ...

class LMOrderedIterator:
    bsz: Any = ...
    bptt: Any = ...
    ext_len: Any = ...
    device: Any = ...
    n_step: Any = ...
    data: Any = ...
    n_batch: Any = ...
    def __init__(self, data: Any, bsz: Any, bptt: Any, device: str = ..., ext_len: Optional[Any] = ...) -> None: ...
    def get_batch(self, i: Any, bptt: Optional[Any] = ...): ...
    def get_fixlen_iter(self, start: int = ...) -> None: ...
    def get_varlen_iter(self, start: int = ..., std: int = ..., min_len: int = ..., max_deviation: int = ...) -> None: ...
    def __iter__(self): ...

class LMShuffledIterator:
    data: Any = ...
    bsz: Any = ...
    bptt: Any = ...
    ext_len: Any = ...
    device: Any = ...
    shuffle: Any = ...
    def __init__(self, data: Any, bsz: Any, bptt: Any, device: str = ..., ext_len: Optional[Any] = ..., shuffle: bool = ...) -> None: ...
    def get_sent_stream(self) -> None: ...
    def stream_iterator(self, sent_stream: Any) -> None: ...
    def __iter__(self) -> None: ...

class LMMultiFileIterator(LMShuffledIterator):
    paths: Any = ...
    vocab: Any = ...
    bsz: Any = ...
    bptt: Any = ...
    ext_len: Any = ...
    device: Any = ...
    shuffle: Any = ...
    def __init__(self, paths: Any, vocab: Any, bsz: Any, bptt: Any, device: str = ..., ext_len: Optional[Any] = ..., shuffle: bool = ...) -> None: ...
    def get_sent_stream(self, path: Any): ...
    def __iter__(self) -> None: ...

class TransfoXLCorpus:
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: Any, cache_dir: Optional[Any] = ..., *inputs: Any, **kwargs: Any): ...
    vocab: Any = ...
    dataset: Any = ...
    train: Any = ...
    valid: Any = ...
    test: Any = ...
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...
    def build_corpus(self, path: Any, dataset: Any) -> None: ...
    def get_iterator(self, split: Any, *args: Any, **kwargs: Any): ...

def get_lm_corpus(datadir: Any, dataset: Any): ...
