# Stubs for pytorch_transformers.modeling_xlm (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .modeling_utils import PreTrainedModel, PretrainedConfig, SQuADHead, SequenceSummary, add_start_docstrings, prune_linear_layer
from torch import nn
from typing import Any, Optional

logger: Any
XLM_PRETRAINED_MODEL_ARCHIVE_MAP: Any
XLM_PRETRAINED_CONFIG_ARCHIVE_MAP: Any

class XLMConfig(PretrainedConfig):
    pretrained_config_archive_map: Any = ...
    n_words: Any = ...
    emb_dim: Any = ...
    n_layers: Any = ...
    n_heads: Any = ...
    dropout: Any = ...
    attention_dropout: Any = ...
    gelu_activation: Any = ...
    sinusoidal_embeddings: Any = ...
    causal: Any = ...
    asm: Any = ...
    n_langs: Any = ...
    layer_norm_eps: Any = ...
    bos_index: Any = ...
    eos_index: Any = ...
    pad_index: Any = ...
    unk_index: Any = ...
    mask_index: Any = ...
    is_encoder: Any = ...
    max_position_embeddings: Any = ...
    embed_init_std: Any = ...
    init_std: Any = ...
    finetuning_task: Any = ...
    num_labels: Any = ...
    summary_type: Any = ...
    summary_use_proj: Any = ...
    summary_activation: Any = ...
    summary_proj_to_labels: Any = ...
    summary_first_dropout: Any = ...
    start_n_top: Any = ...
    end_n_top: Any = ...
    def __init__(self, vocab_size_or_config_json_file: int = ..., emb_dim: int = ..., n_layers: int = ..., n_heads: int = ..., dropout: float = ..., attention_dropout: float = ..., gelu_activation: bool = ..., sinusoidal_embeddings: bool = ..., causal: bool = ..., asm: bool = ..., n_langs: int = ..., max_position_embeddings: int = ..., embed_init_std: Any = ..., layer_norm_eps: float = ..., init_std: float = ..., bos_index: int = ..., eos_index: int = ..., pad_index: int = ..., unk_index: int = ..., mask_index: int = ..., is_encoder: bool = ..., finetuning_task: Optional[Any] = ..., num_labels: int = ..., summary_type: str = ..., summary_use_proj: bool = ..., summary_activation: Optional[Any] = ..., summary_proj_to_labels: bool = ..., summary_first_dropout: float = ..., start_n_top: int = ..., end_n_top: int = ..., **kwargs: Any) -> None: ...
    @property
    def vocab_size(self): ...
    @vocab_size.setter
    def vocab_size(self, value: Any) -> None: ...
    @property
    def hidden_size(self): ...
    @property
    def num_attention_heads(self): ...
    @property
    def num_hidden_layers(self): ...

def create_sinusoidal_embeddings(n_pos: Any, dim: Any, out: Any) -> None: ...
def gelu(x: Any): ...
def get_masks(slen: Any, lengths: Any, causal: Any, padding_mask: Optional[Any] = ...): ...

class MultiHeadAttention(nn.Module):
    NEW_ID: Any = ...
    layer_id: Any = ...
    output_attentions: Any = ...
    dim: Any = ...
    n_heads: Any = ...
    dropout: Any = ...
    q_lin: Any = ...
    k_lin: Any = ...
    v_lin: Any = ...
    out_lin: Any = ...
    def __init__(self, n_heads: Any, dim: Any, config: Any) -> None: ...
    def prune_heads(self, heads: Any) -> None: ...
    def forward(self, input: Any, mask: Any, kv: Optional[Any] = ..., cache: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class TransformerFFN(nn.Module):
    dropout: Any = ...
    lin1: Any = ...
    lin2: Any = ...
    act: Any = ...
    def __init__(self, in_dim: Any, dim_hidden: Any, out_dim: Any, config: Any) -> None: ...
    def forward(self, input: Any): ...

class XLMPreTrainedModel(PreTrainedModel):
    config_class: Any = ...
    pretrained_model_archive_map: Any = ...
    load_tf_weights: Any = ...
    base_model_prefix: str = ...
    def __init__(self, *inputs: Any, **kwargs: Any) -> None: ...
    def init_weights(self, module: Any) -> None: ...

XLM_START_DOCSTRING: str
XLM_INPUTS_DOCSTRING: str

class XLMModel(XLMPreTrainedModel):
    ATTRIBUTES: Any = ...
    output_attentions: Any = ...
    output_hidden_states: Any = ...
    is_encoder: Any = ...
    is_decoder: Any = ...
    causal: Any = ...
    n_langs: Any = ...
    n_words: Any = ...
    eos_index: Any = ...
    pad_index: Any = ...
    dim: Any = ...
    hidden_dim: Any = ...
    n_heads: Any = ...
    n_layers: Any = ...
    dropout: Any = ...
    attention_dropout: Any = ...
    position_embeddings: Any = ...
    lang_embeddings: Any = ...
    embeddings: Any = ...
    layer_norm_emb: Any = ...
    attentions: Any = ...
    layer_norm1: Any = ...
    ffns: Any = ...
    layer_norm2: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, lengths: Optional[Any] = ..., position_ids: Optional[Any] = ..., langs: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., cache: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class XLMPredLayer(nn.Module):
    asm: Any = ...
    n_words: Any = ...
    pad_index: Any = ...
    proj: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, x: Any, y: Optional[Any] = ...): ...

class XLMWithLMHeadModel(XLMPreTrainedModel):
    transformer: Any = ...
    pred_layer: Any = ...
    def __init__(self, config: Any) -> None: ...
    def tie_weights(self) -> None: ...
    def forward(self, input_ids: Any, lengths: Optional[Any] = ..., position_ids: Optional[Any] = ..., langs: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., cache: Optional[Any] = ..., labels: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class XLMForSequenceClassification(XLMPreTrainedModel):
    num_labels: Any = ...
    transformer: Any = ...
    sequence_summary: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, lengths: Optional[Any] = ..., position_ids: Optional[Any] = ..., langs: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., cache: Optional[Any] = ..., labels: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class XLMForQuestionAnswering(XLMPreTrainedModel):
    transformer: Any = ...
    qa_outputs: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, lengths: Optional[Any] = ..., position_ids: Optional[Any] = ..., langs: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., cache: Optional[Any] = ..., start_positions: Optional[Any] = ..., end_positions: Optional[Any] = ..., cls_index: Optional[Any] = ..., is_impossible: Optional[Any] = ..., p_mask: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...
