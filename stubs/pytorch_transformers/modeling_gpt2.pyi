# Stubs for pytorch_transformers.modeling_gpt2 (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import torch.nn as nn
from .modeling_utils import CONFIG_NAME, Conv1D, PreTrainedModel, PretrainedConfig, SequenceSummary, WEIGHTS_NAME, add_start_docstrings, prune_conv1d_layer
from typing import Any, Optional

logger: Any
GPT2_PRETRAINED_MODEL_ARCHIVE_MAP: Any
GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP: Any

def load_tf_weights_in_gpt2(model: Any, config: Any, gpt2_checkpoint_path: Any): ...
def gelu(x: Any): ...

class GPT2Config(PretrainedConfig):
    pretrained_config_archive_map: Any = ...
    vocab_size: Any = ...
    n_ctx: Any = ...
    n_positions: Any = ...
    n_embd: Any = ...
    n_layer: Any = ...
    n_head: Any = ...
    resid_pdrop: Any = ...
    embd_pdrop: Any = ...
    attn_pdrop: Any = ...
    layer_norm_epsilon: Any = ...
    initializer_range: Any = ...
    num_labels: Any = ...
    summary_type: Any = ...
    summary_use_proj: Any = ...
    summary_activation: Any = ...
    summary_first_dropout: Any = ...
    summary_proj_to_labels: Any = ...
    def __init__(self, vocab_size_or_config_json_file: int = ..., n_positions: int = ..., n_ctx: int = ..., n_embd: int = ..., n_layer: int = ..., n_head: int = ..., resid_pdrop: float = ..., embd_pdrop: float = ..., attn_pdrop: float = ..., layer_norm_epsilon: float = ..., initializer_range: float = ..., num_labels: int = ..., summary_type: str = ..., summary_use_proj: bool = ..., summary_activation: Optional[Any] = ..., summary_proj_to_labels: bool = ..., summary_first_dropout: float = ..., **kwargs: Any) -> None: ...
    @property
    def max_position_embeddings(self): ...
    @property
    def hidden_size(self): ...
    @property
    def num_attention_heads(self): ...
    @property
    def num_hidden_layers(self): ...

class Attention(nn.Module):
    output_attentions: Any = ...
    n_head: Any = ...
    split_size: Any = ...
    scale: Any = ...
    c_attn: Any = ...
    c_proj: Any = ...
    attn_dropout: Any = ...
    resid_dropout: Any = ...
    def __init__(self, nx: Any, n_ctx: Any, config: Any, scale: bool = ...) -> None: ...
    def prune_heads(self, heads: Any) -> None: ...
    def merge_heads(self, x: Any): ...
    def split_heads(self, x: Any, k: bool = ...): ...
    def forward(self, x: Any, layer_past: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class MLP(nn.Module):
    c_fc: Any = ...
    c_proj: Any = ...
    act: Any = ...
    dropout: Any = ...
    def __init__(self, n_state: Any, config: Any) -> None: ...
    def forward(self, x: Any): ...

class Block(nn.Module):
    ln_1: Any = ...
    attn: Any = ...
    ln_2: Any = ...
    mlp: Any = ...
    def __init__(self, n_ctx: Any, config: Any, scale: bool = ...) -> None: ...
    def forward(self, x: Any, layer_past: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class GPT2PreTrainedModel(PreTrainedModel):
    config_class: Any = ...
    pretrained_model_archive_map: Any = ...
    load_tf_weights: Any = ...
    base_model_prefix: str = ...
    def __init__(self, *inputs: Any, **kwargs: Any) -> None: ...
    def init_weights(self, module: Any) -> None: ...

GPT2_START_DOCSTRING: str
GPT2_INPUTS_DOCSTRING: str

class GPT2Model(GPT2PreTrainedModel):
    output_hidden_states: Any = ...
    output_attentions: Any = ...
    wte: Any = ...
    wpe: Any = ...
    drop: Any = ...
    h: Any = ...
    ln_f: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, position_ids: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., past: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class GPT2LMHeadModel(GPT2PreTrainedModel):
    transformer: Any = ...
    lm_head: Any = ...
    def __init__(self, config: Any) -> None: ...
    def tie_weights(self) -> None: ...
    def forward(self, input_ids: Any, position_ids: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., labels: Optional[Any] = ..., past: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class GPT2DoubleHeadsModel(GPT2PreTrainedModel):
    transformer: Any = ...
    lm_head: Any = ...
    multiple_choice_head: Any = ...
    def __init__(self, config: Any) -> None: ...
    def tie_weights(self) -> None: ...
    def forward(self, input_ids: Any, mc_token_ids: Optional[Any] = ..., lm_labels: Optional[Any] = ..., mc_labels: Optional[Any] = ..., token_type_ids: Optional[Any] = ..., position_ids: Optional[Any] = ..., past: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...
