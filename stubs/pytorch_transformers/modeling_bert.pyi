# Stubs for pytorch_transformers.modeling_bert (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .modeling_utils import CONFIG_NAME, PreTrainedModel, PretrainedConfig, WEIGHTS_NAME, add_start_docstrings, prune_linear_layer
from torch import nn
from typing import Any, Optional

logger: Any
BERT_PRETRAINED_MODEL_ARCHIVE_MAP: Any
BERT_PRETRAINED_CONFIG_ARCHIVE_MAP: Any

def load_tf_weights_in_bert(model: Any, config: Any, tf_checkpoint_path: Any): ...
def gelu(x: Any): ...
def swish(x: Any): ...

ACT2FN: Any

class BertConfig(PretrainedConfig):
    pretrained_config_archive_map: Any = ...
    vocab_size: Any = ...
    hidden_size: Any = ...
    num_hidden_layers: Any = ...
    num_attention_heads: Any = ...
    hidden_act: Any = ...
    intermediate_size: Any = ...
    hidden_dropout_prob: Any = ...
    attention_probs_dropout_prob: Any = ...
    max_position_embeddings: Any = ...
    type_vocab_size: Any = ...
    initializer_range: Any = ...
    layer_norm_eps: Any = ...
    def __init__(self, vocab_size_or_config_json_file: int = ..., hidden_size: int = ..., num_hidden_layers: int = ..., num_attention_heads: int = ..., intermediate_size: int = ..., hidden_act: str = ..., hidden_dropout_prob: float = ..., attention_probs_dropout_prob: float = ..., max_position_embeddings: int = ..., type_vocab_size: int = ..., initializer_range: float = ..., layer_norm_eps: float = ..., **kwargs: Any) -> None: ...

class BertLayerNorm(nn.Module):
    weight: Any = ...
    bias: Any = ...
    variance_epsilon: Any = ...
    def __init__(self, hidden_size: Any, eps: float = ...) -> None: ...
    def forward(self, x: Any): ...

class BertEmbeddings(nn.Module):
    word_embeddings: Any = ...
    position_embeddings: Any = ...
    token_type_embeddings: Any = ...
    LayerNorm: Any = ...
    dropout: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., position_ids: Optional[Any] = ...): ...

class BertSelfAttention(nn.Module):
    output_attentions: Any = ...
    num_attention_heads: Any = ...
    attention_head_size: Any = ...
    all_head_size: Any = ...
    query: Any = ...
    key: Any = ...
    value: Any = ...
    dropout: Any = ...
    def __init__(self, config: Any) -> None: ...
    def transpose_for_scores(self, x: Any): ...
    def forward(self, hidden_states: Any, attention_mask: Any, head_mask: Optional[Any] = ...): ...

class BertSelfOutput(nn.Module):
    dense: Any = ...
    LayerNorm: Any = ...
    dropout: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, input_tensor: Any): ...

class BertAttention(nn.Module):
    self: Any = ...
    output: Any = ...
    def __init__(self, config: Any) -> None: ...
    def prune_heads(self, heads: Any) -> None: ...
    def forward(self, input_tensor: Any, attention_mask: Any, head_mask: Optional[Any] = ...): ...

class BertIntermediate(nn.Module):
    dense: Any = ...
    intermediate_act_fn: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any): ...

class BertOutput(nn.Module):
    dense: Any = ...
    LayerNorm: Any = ...
    dropout: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, input_tensor: Any): ...

class BertLayer(nn.Module):
    attention: Any = ...
    intermediate: Any = ...
    output: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, attention_mask: Any, head_mask: Optional[Any] = ...): ...

class BertEncoder(nn.Module):
    output_attentions: Any = ...
    output_hidden_states: Any = ...
    layer: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, attention_mask: Any, head_mask: Optional[Any] = ...): ...

class BertPooler(nn.Module):
    dense: Any = ...
    activation: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any): ...

class BertPredictionHeadTransform(nn.Module):
    dense: Any = ...
    transform_act_fn: Any = ...
    LayerNorm: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any): ...

class BertLMPredictionHead(nn.Module):
    transform: Any = ...
    decoder: Any = ...
    bias: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any): ...

class BertOnlyMLMHead(nn.Module):
    predictions: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, sequence_output: Any): ...

class BertOnlyNSPHead(nn.Module):
    seq_relationship: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, pooled_output: Any): ...

class BertPreTrainingHeads(nn.Module):
    predictions: Any = ...
    seq_relationship: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, sequence_output: Any, pooled_output: Any): ...

class BertPreTrainedModel(PreTrainedModel):
    config_class: Any = ...
    pretrained_model_archive_map: Any = ...
    load_tf_weights: Any = ...
    base_model_prefix: str = ...
    def __init__(self, *inputs: Any, **kwargs: Any) -> None: ...
    def init_weights(self, module: Any) -> None: ...

BERT_START_DOCSTRING: str
BERT_INPUTS_DOCSTRING: str

class BertModel(BertPreTrainedModel):
    embeddings: Any = ...
    encoder: Any = ...
    pooler: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForPreTraining(BertPreTrainedModel):
    bert: Any = ...
    cls: Any = ...
    def __init__(self, config: Any) -> None: ...
    def tie_weights(self) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., masked_lm_labels: Optional[Any] = ..., next_sentence_label: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForMaskedLM(BertPreTrainedModel):
    bert: Any = ...
    cls: Any = ...
    def __init__(self, config: Any) -> None: ...
    def tie_weights(self) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., masked_lm_labels: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForNextSentencePrediction(BertPreTrainedModel):
    bert: Any = ...
    cls: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., next_sentence_label: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForSequenceClassification(BertPreTrainedModel):
    num_labels: Any = ...
    bert: Any = ...
    dropout: Any = ...
    classifier: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., labels: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForMultipleChoice(BertPreTrainedModel):
    bert: Any = ...
    dropout: Any = ...
    classifier: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., labels: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForTokenClassification(BertPreTrainedModel):
    num_labels: Any = ...
    bert: Any = ...
    dropout: Any = ...
    classifier: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., labels: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...

class BertForQuestionAnswering(BertPreTrainedModel):
    num_labels: Any = ...
    bert: Any = ...
    qa_outputs: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, input_ids: Any, token_type_ids: Optional[Any] = ..., attention_mask: Optional[Any] = ..., start_positions: Optional[Any] = ..., end_positions: Optional[Any] = ..., position_ids: Optional[Any] = ..., head_mask: Optional[Any] = ...): ...
