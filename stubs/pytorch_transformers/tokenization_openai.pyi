# Stubs for pytorch_transformers.tokenization_openai (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .tokenization_bert import BasicTokenizer
from .tokenization_utils import PreTrainedTokenizer
from typing import Any

logger: Any
VOCAB_FILES_NAMES: Any
PRETRAINED_VOCAB_FILES_MAP: Any
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES: Any

def get_pairs(word: Any): ...
def text_standardize(text: Any): ...

class OpenAIGPTTokenizer(PreTrainedTokenizer):
    vocab_files_names: Any = ...
    pretrained_vocab_files_map: Any = ...
    max_model_input_sizes: Any = ...
    nlp: Any = ...
    fix_text: Any = ...
    encoder: Any = ...
    decoder: Any = ...
    bpe_ranks: Any = ...
    cache: Any = ...
    def __init__(self, vocab_file: Any, merges_file: Any, unk_token: str = ..., **kwargs: Any) -> None: ...
    @property
    def vocab_size(self): ...
    def bpe(self, token: Any): ...
    def convert_tokens_to_string(self, tokens: Any): ...
    def save_vocabulary(self, save_directory: Any): ...
