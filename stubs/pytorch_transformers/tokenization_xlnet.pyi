# Stubs for pytorch_transformers.tokenization_xlnet (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .tokenization_utils import PreTrainedTokenizer, clean_up_tokenization
from typing import Any, Optional

logger: Any
VOCAB_FILES_NAMES: Any
PRETRAINED_VOCAB_FILES_MAP: Any
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES: Any
SPIECE_UNDERLINE: str
SEG_ID_A: int
SEG_ID_B: int
SEG_ID_CLS: int
SEG_ID_SEP: int
SEG_ID_PAD: int

class XLNetTokenizer(PreTrainedTokenizer):
    vocab_files_names: Any = ...
    pretrained_vocab_files_map: Any = ...
    max_model_input_sizes: Any = ...
    do_lower_case: Any = ...
    remove_space: Any = ...
    keep_accents: Any = ...
    vocab_file: Any = ...
    sp_model: Any = ...
    def __init__(self, vocab_file: Any, max_len: Optional[Any] = ..., do_lower_case: bool = ..., remove_space: bool = ..., keep_accents: bool = ..., bos_token: str = ..., eos_token: str = ..., unk_token: str = ..., sep_token: str = ..., pad_token: str = ..., cls_token: str = ..., mask_token: str = ..., additional_special_tokens: Any = ..., **kwargs: Any) -> None: ...
    @property
    def vocab_size(self): ...
    def preprocess_text(self, inputs: Any): ...
    def convert_tokens_to_string(self, tokens: Any): ...
    def save_vocabulary(self, save_directory: Any): ...
