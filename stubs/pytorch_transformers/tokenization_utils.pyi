# Stubs for pytorch_transformers.tokenization_utils (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .file_utils import cached_path
from typing import Any, Optional

logger: Any
SPECIAL_TOKENS_MAP_FILE: str
ADDED_TOKENS_FILE: str

class PreTrainedTokenizer:
    vocab_files_names: Any = ...
    pretrained_vocab_files_map: Any = ...
    max_model_input_sizes: Any = ...
    SPECIAL_TOKENS_ATTRIBUTES: Any = ...
    @property
    def bos_token(self): ...
    @property
    def eos_token(self): ...
    @property
    def unk_token(self): ...
    @property
    def sep_token(self): ...
    @property
    def pad_token(self): ...
    @property
    def cls_token(self): ...
    @property
    def mask_token(self): ...
    @property
    def additional_special_tokens(self): ...
    @bos_token.setter
    def bos_token(self, value: Any) -> None: ...
    @eos_token.setter
    def eos_token(self, value: Any) -> None: ...
    @unk_token.setter
    def unk_token(self, value: Any) -> None: ...
    @sep_token.setter
    def sep_token(self, value: Any) -> None: ...
    @pad_token.setter
    def pad_token(self, value: Any) -> None: ...
    @cls_token.setter
    def cls_token(self, value: Any) -> None: ...
    @mask_token.setter
    def mask_token(self, value: Any) -> None: ...
    @additional_special_tokens.setter
    def additional_special_tokens(self, value: Any) -> None: ...
    max_len: Any = ...
    added_tokens_encoder: Any = ...
    added_tokens_decoder: Any = ...
    def __init__(self, max_len: Optional[Any] = ..., **kwargs: Any) -> None: ...
    @classmethod
    def from_pretrained(cls, *inputs: Any, **kwargs: Any): ...
    def save_pretrained(self, save_directory: Any): ...
    def save_vocabulary(self, save_directory: Any) -> None: ...
    def vocab_size(self) -> None: ...
    def __len__(self): ...
    def add_tokens(self, new_tokens: Any): ...
    def add_special_tokens(self, special_tokens_dict: Any): ...
    def tokenize(self, text: Any, **kwargs: Any): ...
    def convert_tokens_to_ids(self, tokens: Any): ...
    def encode(self, text: Any): ...
    def convert_ids_to_tokens(self, ids: Any, skip_special_tokens: bool = ...): ...
    def convert_tokens_to_string(self, tokens: Any): ...
    def decode(self, token_ids: Any, skip_special_tokens: bool = ..., clean_up_tokenization_spaces: bool = ...): ...
    @property
    def special_tokens_map(self): ...
    @property
    def all_special_tokens(self): ...
    @property
    def all_special_ids(self): ...

def clean_up_tokenization(out_string: Any): ...
