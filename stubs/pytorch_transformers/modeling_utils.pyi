# Stubs for pytorch_transformers.modeling_utils (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .file_utils import cached_path
from torch import nn
from typing import Any, Optional

logger: Any
CONFIG_NAME: str
WEIGHTS_NAME: str
TF_WEIGHTS_NAME: str

def add_start_docstrings(*docstr: Any): ...

class PretrainedConfig:
    pretrained_config_archive_map: Any = ...
    finetuning_task: Any = ...
    num_labels: Any = ...
    output_attentions: Any = ...
    output_hidden_states: Any = ...
    torchscript: Any = ...
    def __init__(self, **kwargs: Any) -> None: ...
    def save_pretrained(self, save_directory: Any) -> None: ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: Any, *input: Any, **kwargs: Any): ...
    @classmethod
    def from_dict(cls, json_object: Any): ...
    @classmethod
    def from_json_file(cls, json_file: Any): ...
    def __eq__(self, other: Any): ...
    def to_dict(self): ...
    def to_json_string(self): ...
    def to_json_file(self, json_file_path: Any) -> None: ...

class PreTrainedModel(nn.Module):
    config_class: Any = ...
    pretrained_model_archive_map: Any = ...
    load_tf_weights: Any = ...
    base_model_prefix: str = ...
    input_embeddings: Any = ...
    config: Any = ...
    def __init__(self, config: Any, *inputs: Any, **kwargs: Any) -> None: ...
    def resize_token_embeddings(self, new_num_tokens: Optional[Any] = ...): ...
    def prune_heads(self, heads_to_prune: Any) -> None: ...
    def save_pretrained(self, save_directory: Any) -> None: ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: Any, *inputs: Any, **kwargs: Any): ...

class Conv1D(nn.Module):
    nf: Any = ...
    weight: Any = ...
    bias: Any = ...
    def __init__(self, nf: Any, nx: Any) -> None: ...
    def forward(self, x: Any): ...

class PoolerStartLogits(nn.Module):
    dense: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, p_mask: Optional[Any] = ...): ...

class PoolerEndLogits(nn.Module):
    dense_0: Any = ...
    activation: Any = ...
    LayerNorm: Any = ...
    dense_1: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, start_states: Optional[Any] = ..., start_positions: Optional[Any] = ..., p_mask: Optional[Any] = ...): ...

class PoolerAnswerClass(nn.Module):
    dense_0: Any = ...
    activation: Any = ...
    dense_1: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, start_states: Optional[Any] = ..., start_positions: Optional[Any] = ..., cls_index: Optional[Any] = ...): ...

class SQuADHead(nn.Module):
    start_n_top: Any = ...
    end_n_top: Any = ...
    start_logits: Any = ...
    end_logits: Any = ...
    answer_class: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, start_positions: Optional[Any] = ..., end_positions: Optional[Any] = ..., cls_index: Optional[Any] = ..., is_impossible: Optional[Any] = ..., p_mask: Optional[Any] = ...): ...

class SequenceSummary(nn.Module):
    summary_type: Any = ...
    summary: Any = ...
    activation: Any = ...
    first_dropout: Any = ...
    last_dropout: Any = ...
    def __init__(self, config: Any) -> None: ...
    def forward(self, hidden_states: Any, token_ids: Optional[Any] = ...): ...

def prune_linear_layer(layer: Any, index: Any, dim: int = ...): ...
def prune_conv1d_layer(layer: Any, index: Any, dim: int = ...): ...
def prune_layer(layer: Any, index: Any, dim: Optional[Any] = ...): ...
