# Stubs for torch.optim.adam (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .optimizer import Optimizer
from typing import Any, Optional

class Adam(Optimizer):
    def __init__(self, params: Any, lr: float = ..., betas: Any = ..., eps: float = ..., weight_decay: int = ..., amsgrad: bool = ...) -> None: ...
    def step(self, closure: Optional[Any] = ...): ...
