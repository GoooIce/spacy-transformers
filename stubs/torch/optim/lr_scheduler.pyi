# Stubs for torch.optim.lr_scheduler (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .optimizer import Optimizer
from typing import Any, Optional

class _LRScheduler:
    optimizer: Any = ...
    base_lrs: Any = ...
    def __init__(self, optimizer: Any, last_epoch: int = ...) -> None: ...
    def state_dict(self): ...
    def load_state_dict(self, state_dict: Any) -> None: ...
    def get_lr(self) -> None: ...
    last_epoch: Any = ...
    def step(self, epoch: Optional[Any] = ...) -> None: ...

class LambdaLR(_LRScheduler):
    optimizer: Any = ...
    lr_lambdas: Any = ...
    last_epoch: Any = ...
    def __init__(self, optimizer: Any, lr_lambda: Any, last_epoch: int = ...) -> None: ...
    def state_dict(self): ...
    def load_state_dict(self, state_dict: Any) -> None: ...
    def get_lr(self): ...

class StepLR(_LRScheduler):
    step_size: Any = ...
    gamma: Any = ...
    def __init__(self, optimizer: Any, step_size: Any, gamma: float = ..., last_epoch: int = ...) -> None: ...
    def get_lr(self): ...

class MultiStepLR(_LRScheduler):
    milestones: Any = ...
    gamma: Any = ...
    def __init__(self, optimizer: Any, milestones: Any, gamma: float = ..., last_epoch: int = ...) -> None: ...
    def get_lr(self): ...

class ExponentialLR(_LRScheduler):
    gamma: Any = ...
    def __init__(self, optimizer: Any, gamma: Any, last_epoch: int = ...) -> None: ...
    def get_lr(self): ...

class CosineAnnealingLR(_LRScheduler):
    T_max: Any = ...
    eta_min: Any = ...
    def __init__(self, optimizer: Any, T_max: Any, eta_min: int = ..., last_epoch: int = ...) -> None: ...
    def get_lr(self): ...

class ReduceLROnPlateau:
    factor: Any = ...
    optimizer: Any = ...
    min_lrs: Any = ...
    patience: Any = ...
    verbose: Any = ...
    cooldown: Any = ...
    cooldown_counter: int = ...
    mode: Any = ...
    threshold: Any = ...
    threshold_mode: Any = ...
    best: Any = ...
    num_bad_epochs: Any = ...
    mode_worse: Any = ...
    is_better: Any = ...
    eps: Any = ...
    last_epoch: int = ...
    def __init__(self, optimizer: Any, mode: str = ..., factor: float = ..., patience: int = ..., verbose: bool = ..., threshold: float = ..., threshold_mode: str = ..., cooldown: int = ..., min_lr: int = ..., eps: float = ...) -> None: ...
    def step(self, metrics: Any, epoch: Optional[Any] = ...) -> None: ...
    @property
    def in_cooldown(self): ...
    def state_dict(self): ...
    def load_state_dict(self, state_dict: Any) -> None: ...

class CyclicLR(_LRScheduler):
    optimizer: Any = ...
    max_lrs: Any = ...
    total_size: Any = ...
    step_ratio: Any = ...
    mode: Any = ...
    gamma: Any = ...
    scale_fn: Any = ...
    scale_mode: str = ...
    cycle_momentum: Any = ...
    base_momentums: Any = ...
    max_momentums: Any = ...
    def __init__(self, optimizer: Any, base_lr: Any, max_lr: Any, step_size_up: int = ..., step_size_down: Optional[Any] = ..., mode: str = ..., gamma: float = ..., scale_fn: Optional[Any] = ..., scale_mode: str = ..., cycle_momentum: bool = ..., base_momentum: float = ..., max_momentum: float = ..., last_epoch: int = ...) -> None: ...
    def get_lr(self): ...

class CosineAnnealingWarmRestarts(_LRScheduler):
    T_0: Any = ...
    T_i: Any = ...
    T_mult: Any = ...
    eta_min: Any = ...
    T_cur: Any = ...
    def __init__(self, optimizer: Any, T_0: Any, T_mult: int = ..., eta_min: int = ..., last_epoch: int = ...) -> None: ...
    def get_lr(self): ...
    last_epoch: Any = ...
    def step(self, epoch: Optional[Any] = ...) -> None: ...
