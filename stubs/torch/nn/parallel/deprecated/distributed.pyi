# Stubs for torch.nn.parallel.deprecated.distributed (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from ...modules import Module
from ..parallel_apply import parallel_apply
from ..replicate import replicate
from ..scatter_gather import gather, scatter_kwargs
from typing import Any, Optional

class DistributedDataParallel(Module):
    dim: Any = ...
    module: Any = ...
    device_ids: Any = ...
    output_device: Any = ...
    broadcast_buffers: Any = ...
    need_reduction: bool = ...
    broadcast_bucket_size: Any = ...
    nccl_reduce_bucket_size: Any = ...
    bucket_sizes: Any = ...
    bucket_map: Any = ...
    buckets: Any = ...
    bucket_events: Any = ...
    reduced: Any = ...
    dispatch_lock: Any = ...
    def __init__(self, module: Any, device_ids: Optional[Any] = ..., output_device: Optional[Any] = ..., dim: int = ..., broadcast_buffers: bool = ...) -> None: ...
    def forward(self, *inputs: Any, **kwargs: Any): ...
    def scatter(self, inputs: Any, kwargs: Any, device_ids: Any): ...
    def parallel_apply(self, replicas: Any, inputs: Any, kwargs: Any): ...
    def gather(self, outputs: Any, output_device: Any): ...
    def train(self, mode: bool = ...) -> None: ...
