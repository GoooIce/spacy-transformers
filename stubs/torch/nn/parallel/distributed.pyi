# Stubs for torch.nn.parallel.distributed (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from ..modules import Module
from .parallel_apply import parallel_apply
from .replicate import replicate
from .scatter_gather import gather, scatter_kwargs
from typing import Any, Optional

class DistributedDataParallel(Module):
    is_multi_device_module: Any = ...
    is_cuda: Any = ...
    device_ids: Any = ...
    output_device: Any = ...
    process_group: Any = ...
    dim: Any = ...
    module: Any = ...
    broadcast_buffers: Any = ...
    find_unused_parameters: Any = ...
    broadcast_bucket_size: Any = ...
    bucket_bytes_cap: Any = ...
    def __init__(self, module: Any, device_ids: Optional[Any] = ..., output_device: Optional[Any] = ..., dim: int = ..., broadcast_buffers: bool = ..., process_group: Optional[Any] = ..., bucket_cap_mb: int = ..., find_unused_parameters: bool = ..., check_reduction: bool = ...) -> None: ...
    def forward(self, *inputs: Any, **kwargs: Any): ...
    def scatter(self, inputs: Any, kwargs: Any, device_ids: Any): ...
    def parallel_apply(self, replicas: Any, inputs: Any, kwargs: Any): ...
    def gather(self, outputs: Any, output_device: Any): ...
    def train(self, mode: bool = ...) -> None: ...
