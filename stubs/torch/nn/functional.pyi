# Stubs for torch.nn.functional (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from .._jit_internal import List, weak_script
from ._functions import vision
from .modules import utils
from .modules.utils import _list_with_default, _pair, _single, _triple
from typing import Any, Optional

conv1d: Any
conv2d: Any
conv3d: Any
conv_transpose1d: Any
conv_transpose2d: Any
conv_transpose3d: Any
conv_tbc: Any
avg_pool1d: Any
avg_pool2d: Any
avg_pool3d: Any

def fractional_max_pool2d_with_indices(input: Tensor, kernel_size: BroadcastingList2[int], output_size: Optional[BroadcastingList2[int]]=..., output_ratio: Optional[BroadcastingList2[float]]=..., return_indices: bool=..., _random_samples: Optional[Tensor]=...) -> Tuple[Tensor, Tensor]: ...

fractional_max_pool2d: Any

def fractional_max_pool3d_with_indices(input: Tensor, kernel_size: BroadcastingList3[int], output_size: Optional[BroadcastingList3[int]]=..., output_ratio: Optional[BroadcastingList3[float]]=..., return_indices: bool=..., _random_samples: Optional[Tensor]=...) -> Tuple[Tensor, Tensor]: ...

fractional_max_pool3d: Any

def max_pool1d_with_indices(input: Tensor, kernel_size: BroadcastingList1[int], stride: Optional[BroadcastingList1[int]]=..., padding: BroadcastingList1[int]=..., dilation: BroadcastingList1[int]=..., ceil_mode: bool=..., return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

max_pool1d: Any

def max_pool2d_with_indices(input: Tensor, kernel_size: BroadcastingList2[int], stride: Optional[BroadcastingList2[int]]=..., padding: BroadcastingList2[int]=..., dilation: BroadcastingList2[int]=..., ceil_mode: bool=..., return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

max_pool2d: Any

def max_pool3d_with_indices(input: Tensor, kernel_size: BroadcastingList3[int], stride: Optional[BroadcastingList3[int]]=..., padding: BroadcastingList3[int]=..., dilation: BroadcastingList3[int]=..., ceil_mode: bool=..., return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

max_pool3d: Any

def max_unpool1d(input: Tensor, indices: Tensor, kernel_size: BroadcastingList1[int], stride: Optional[BroadcastingList1[int]]=..., padding: BroadcastingList1[int]=..., output_size: Optional[BroadcastingList1[int]]=...) -> Tensor: ...
def max_unpool2d(input: Tensor, indices: Tensor, kernel_size: BroadcastingList2[int], stride: Optional[BroadcastingList2[int]]=..., padding: BroadcastingList2[int]=..., output_size: Optional[BroadcastingList2[int]]=...) -> Tensor: ...
def max_unpool3d(input: Tensor, indices: Tensor, kernel_size: BroadcastingList3[int], stride: Optional[BroadcastingList3[int]]=..., padding: BroadcastingList3[int]=..., output_size: Optional[BroadcastingList3[int]]=...) -> Tensor: ...
def lp_pool2d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[BroadcastingList2[int]]=..., ceil_mode: bool=...) -> Tensor: ...
def lp_pool1d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[BroadcastingList1[int]]=..., ceil_mode: bool=...) -> Tensor: ...
def adaptive_max_pool1d_with_indices(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

adaptive_max_pool1d: Any

def adaptive_max_pool2d_with_indices(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

adaptive_max_pool2d: Any

def adaptive_max_pool3d_with_indices(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool=...) -> Tuple[Tensor, Tensor]: ...

adaptive_max_pool3d: Any
adaptive_avg_pool1d: Any

def adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor: ...
def adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor: ...
def dropout(input: Tensor, p: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...
def alpha_dropout(input: Tensor, p: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...
def dropout2d(input: Tensor, p: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...
def dropout3d(input: Tensor, p: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...
def feature_alpha_dropout(input: Tensor, p: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...
def threshold(input: Tensor, threshold: float, value: float, inplace: bool=...) -> Tensor: ...

threshold_: Any

def relu(input: Tensor, inplace: bool=...) -> Tensor: ...

relu_: Any

def glu(input: Tensor, dim: int=...) -> Tensor: ...
def hardtanh(input: Tensor, min_val: float=..., max_val: float=..., inplace: bool=...) -> Tensor: ...

hardtanh_: Any

def relu6(input: Tensor, inplace: bool=...) -> Tensor: ...
def elu(input: Tensor, alpha: float=..., inplace: bool=...) -> Tensor: ...

elu_: Any

def selu(input: Tensor, inplace: bool=...) -> Tensor: ...

selu_: Any

def celu(input: Tensor, alpha: float=..., inplace: bool=...) -> Tensor: ...

celu_: Any

def leaky_relu(input: Tensor, negative_slope: float=..., inplace: bool=...) -> Tensor: ...

leaky_relu_: Any

def prelu(input: Tensor, weight: Tensor) -> Tensor: ...
def rrelu(input: Tensor, lower: float=..., upper: float=..., training: bool=..., inplace: bool=...) -> Tensor: ...

rrelu_: Any
logsigmoid: Any

def hardshrink(input: Tensor, lambd: float=...) -> Tensor: ...
def tanhshrink(input: Any): ...
def softsign(input: Any): ...

softplus: Any

def softmin(input: Tensor, dim: Optional[int]=..., _stacklevel: int=..., dtype: Optional[int]=...) -> Tensor: ...
def softmax(input: Tensor, dim: Optional[int]=..., _stacklevel: int=..., dtype: Optional[int]=...) -> Tensor: ...
def gumbel_softmax(logits: Tensor, tau: float=..., hard: bool=..., eps: float=..., dim: int=...) -> Tensor: ...
def log_softmax(input: Tensor, dim: Optional[int]=..., _stacklevel: int=..., dtype: Optional[int]=...) -> Tensor: ...

softshrink: Any

def tanh(input: Any): ...
def sigmoid(input: Any): ...
def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor]=...) -> Tensor: ...
def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor]=...) -> Tensor: ...
def embedding(input: Tensor, weight: Tensor, padding_idx: Optional[int]=..., max_norm: Optional[float]=..., norm_type: float=..., scale_grad_by_freq: bool=..., sparse: bool=...) -> Tensor: ...
def embedding_bag(input: Tensor, weight: Tensor, offsets: Optional[Tensor]=..., max_norm: Optional[float]=..., norm_type: float=..., scale_grad_by_freq: bool=..., mode: str=..., sparse: bool=..., per_sample_weights: Optional[Tensor]=...) -> Tensor: ...
def batch_norm(input: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], weight: Optional[Tensor]=..., bias: Optional[Tensor]=..., training: bool=..., momentum: float=..., eps: float=...) -> Tensor: ...
def instance_norm(input: Tensor, running_mean: Optional[Tensor]=..., running_var: Optional[Tensor]=..., weight: Optional[Tensor]=..., bias: Optional[Tensor]=..., use_input_stats: bool=..., momentum: float=..., eps: float=...) -> Tensor: ...
def layer_norm(input: Tensor, normalized_shape: List[int], weight: Optional[Tensor]=..., bias: Optional[Tensor]=..., eps: float=...) -> Tensor: ...
def group_norm(input: Tensor, num_groups: int, weight: Optional[Tensor]=..., bias: Optional[Tensor]=..., eps: float=...) -> Tensor: ...
def local_response_norm(input: Tensor, size: int, alpha: float=..., beta: float=..., k: float=...) -> Tensor: ...
def ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: int=..., reduction: str=..., zero_infinity: bool=...) -> Tensor: ...
def nll_loss(input: Tensor, target: Tensor, weight: Optional[Tensor]=..., size_average: Optional[bool]=..., ignore_index: int=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def poisson_nll_loss(input: Tensor, target: Tensor, log_input: bool=..., full: bool=..., size_average: Optional[bool]=..., eps: float=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def kl_div(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor]=..., size_average: Optional[bool]=..., ignore_index: int=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def binary_cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor]=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def binary_cross_entropy_with_logits(input: Tensor, target: Tensor, weight: Optional[Tensor]=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=..., pos_weight: Optional[Tensor]=...) -> Tensor: ...
def smooth_l1_loss(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def l1_loss(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def mse_loss(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def margin_ranking_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def hinge_embedding_loss(input: Tensor, target: Tensor, margin: float=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def multilabel_margin_loss(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def soft_margin_loss(input: Tensor, target: Tensor, size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def multilabel_soft_margin_loss(input: Tensor, target: Tensor, weight: Optional[Tensor]=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def cosine_embedding_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def multi_margin_loss(input: Tensor, target: Tensor, p: int=..., margin: float=..., weight: Optional[Tensor]=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...

pixel_shuffle: Any

def upsample(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ..., mode: str = ..., align_corners: Optional[Any] = ...): ...
def interpolate(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ..., mode: str = ..., align_corners: Optional[Any] = ...): ...
def upsample_nearest(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ...): ...
def upsample_bilinear(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ...): ...

GRID_SAMPLE_INTERPOLATION_MODES: Any
GRID_SAMPLE_PADDING_MODES: Any

def grid_sample(input: Tensor, grid: Tensor, mode: str=..., padding_mode: str=...) -> Tensor: ...
def affine_grid(theta: Tensor, size: List[int]) -> Tensor: ...
def pad(input: Tensor, pad: List[int], mode: str=..., value: float=...) -> Tensor: ...
def pairwise_distance(x1: Tensor, x2: Tensor, p: float=..., eps: float=..., keepdim: bool=...) -> Tensor: ...

pdist: Any
cosine_similarity: Any
one_hot: Any

def triplet_margin_loss(anchor: Tensor, positive: Tensor, negative: Tensor, margin: float=..., p: float=..., eps: float=..., swap: bool=..., size_average: Optional[bool]=..., reduce: Optional[bool]=..., reduction: str=...) -> Tensor: ...
def normalize(input: Tensor, p: float=..., dim: int=..., eps: float=..., out: Optional[Tensor]=...) -> Tensor: ...
def assert_int_or_pair(arg: Any, arg_name: Any, message: Any) -> None: ...
def unfold(input: Tensor, kernel_size: BroadcastingList2[int], dilation: BroadcastingList2[int]=..., padding: BroadcastingList2[int]=..., stride: BroadcastingList2[int]=...) -> Tensor: ...
def fold(input: Tensor, output_size: BroadcastingList2[int], kernel_size: BroadcastingList2[int], dilation: BroadcastingList2[int]=..., padding: BroadcastingList2[int]=..., stride: BroadcastingList2[int]=...) -> Tensor: ...
