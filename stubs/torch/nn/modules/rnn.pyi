# Stubs for torch.nn.modules.rnn (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from ..._jit_internal import _parameter_list, weak_module, weak_script, weak_script_method
from ..parameter import Parameter
from ..utils.rnn import PackedSequence, get_packed_sequence
from .module import Module
from typing import Any, Optional

def apply_permutation(tensor: Tensor, permutation: Tensor, dim: int=...) -> Tensor: ...

class RNNBase(Module):
    __constants__: Any = ...
    mode: Any = ...
    input_size: Any = ...
    hidden_size: Any = ...
    num_layers: Any = ...
    bias: Any = ...
    batch_first: Any = ...
    dropout: Any = ...
    bidirectional: Any = ...
    def __init__(self, mode: Any, input_size: Any, hidden_size: Any, num_layers: int = ..., bias: bool = ..., batch_first: bool = ..., dropout: float = ..., bidirectional: bool = ...) -> None: ...
    def flatten_parameters(self) -> None: ...
    def reset_parameters(self) -> None: ...
    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None: ...
    def get_expected_hidden_size(self, input: Tensor, batch_sizes: Optional[Tensor]) -> Tuple[int, int, int]: ...
    def check_hidden_size(self, hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str=...) -> None: ...
    def check_forward_args(self, input: Any, hidden: Any, batch_sizes: Any) -> None: ...
    def permute_hidden(self, hx: Any, permutation: Any): ...
    def forward(self, input: Any, hx: Optional[Any] = ...): ...
    def extra_repr(self): ...
    @property
    def all_weights(self): ...

class RNN(RNNBase):
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...

class LSTM(RNNBase):
    __overloads__: Any = ...
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...
    def check_forward_args(self, input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]) -> None: ...
    def permute_hidden(self, hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> Tuple[Tensor, Tensor]: ...
    def forward_impl(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]], batch_sizes: Optional[Tensor], max_batch_size: int, sorted_indices: Optional[Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]: ...
    def forward_tensor(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]=...) -> Tuple[Tensor, Tuple[Tensor, Tensor]]: ...
    def forward_packed(self, input: Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]], hx: Optional[Tuple[Tensor, Tensor]]=...) -> Tuple[Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]], Tuple[Tensor, Tensor]]: ...
    def forward(self, input: Any, hx: Optional[Any] = ...): ...

class GRU(RNNBase):
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...

class RNNCellBase(Module):
    __constants__: Any = ...
    input_size: Any = ...
    hidden_size: Any = ...
    bias: Any = ...
    weight_ih: Any = ...
    weight_hh: Any = ...
    bias_ih: Any = ...
    bias_hh: Any = ...
    def __init__(self, input_size: Any, hidden_size: Any, bias: Any, num_chunks: Any) -> None: ...
    def extra_repr(self): ...
    def check_forward_input(self, input: Any) -> None: ...
    def check_forward_hidden(self, input: Tensor, hx: Tensor, hidden_label: str=...) -> None: ...
    def reset_parameters(self) -> None: ...

class RNNCell(RNNCellBase):
    __constants__: Any = ...
    nonlinearity: Any = ...
    def __init__(self, input_size: Any, hidden_size: Any, bias: bool = ..., nonlinearity: str = ...) -> None: ...
    def forward(self, input: Tensor, hx: Optional[Tensor]=...) -> Tensor: ...

class LSTMCell(RNNCellBase):
    def __init__(self, input_size: Any, hidden_size: Any, bias: bool = ...) -> None: ...
    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]]=...) -> Tuple[Tensor, Tensor]: ...

class GRUCell(RNNCellBase):
    def __init__(self, input_size: Any, hidden_size: Any, bias: bool = ...) -> None: ...
    def forward(self, input: Tensor, hx: Optional[Tensor]=...) -> Tensor: ...
