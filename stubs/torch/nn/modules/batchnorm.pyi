# Stubs for torch.nn.modules.batchnorm (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from ..._jit_internal import weak_module, weak_script_method
from .module import Module
from typing import Any, Optional

class _BatchNorm(Module):
    __constants__: Any = ...
    num_features: Any = ...
    eps: Any = ...
    momentum: Any = ...
    affine: Any = ...
    track_running_stats: Any = ...
    weight: Any = ...
    bias: Any = ...
    def __init__(self, num_features: Any, eps: float = ..., momentum: float = ..., affine: bool = ..., track_running_stats: bool = ...) -> None: ...
    def reset_running_stats(self) -> None: ...
    def reset_parameters(self) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class BatchNorm1d(_BatchNorm): ...
class BatchNorm2d(_BatchNorm): ...
class BatchNorm3d(_BatchNorm): ...

class SyncBatchNorm(_BatchNorm):
    process_group: Any = ...
    ddp_gpu_size: Any = ...
    def __init__(self, num_features: Any, eps: float = ..., momentum: float = ..., affine: bool = ..., track_running_stats: bool = ..., process_group: Optional[Any] = ...) -> None: ...
    def forward(self, input: Any): ...
    @classmethod
    def convert_sync_batchnorm(cls, module: Any, process_group: Optional[Any] = ...): ...
