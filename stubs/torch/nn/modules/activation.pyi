# Stubs for torch.nn.modules.activation (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from ..._jit_internal import weak_module, weak_script_method
from .module import Module
from typing import Any, Optional

class Threshold(Module):
    __constants__: Any = ...
    threshold: Any = ...
    value: Any = ...
    inplace: Any = ...
    def __init__(self, threshold: Any, value: Any, inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class ReLU(Module):
    __constants__: Any = ...
    inplace: Any = ...
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class RReLU(Module):
    __constants__: Any = ...
    lower: Any = ...
    upper: Any = ...
    inplace: Any = ...
    def __init__(self, lower: Any = ..., upper: Any = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class Hardtanh(Module):
    __constants__: Any = ...
    min_val: Any = ...
    max_val: Any = ...
    inplace: Any = ...
    def __init__(self, min_val: Any = ..., max_val: float = ..., inplace: bool = ..., min_value: Optional[Any] = ..., max_value: Optional[Any] = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class ReLU6(Hardtanh):
    def __init__(self, inplace: bool = ...) -> None: ...
    def extra_repr(self): ...

class Sigmoid(Module):
    def forward(self, input: Any): ...

class Tanh(Module):
    def forward(self, input: Any): ...

class ELU(Module):
    __constants__: Any = ...
    alpha: Any = ...
    inplace: Any = ...
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class CELU(Module):
    __constants__: Any = ...
    alpha: Any = ...
    inplace: Any = ...
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class SELU(Module):
    __constants__: Any = ...
    inplace: Any = ...
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class GLU(Module):
    __constants__: Any = ...
    dim: Any = ...
    def __init__(self, dim: int = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class Hardshrink(Module):
    __constants__: Any = ...
    lambd: Any = ...
    def __init__(self, lambd: float = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class LeakyReLU(Module):
    __constants__: Any = ...
    negative_slope: Any = ...
    inplace: Any = ...
    def __init__(self, negative_slope: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class LogSigmoid(Module):
    def forward(self, input: Any): ...

class Softplus(Module):
    __constants__: Any = ...
    beta: Any = ...
    threshold: Any = ...
    def __init__(self, beta: int = ..., threshold: int = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class Softshrink(Module):
    __constants__: Any = ...
    lambd: Any = ...
    def __init__(self, lambd: float = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class MultiheadAttention(Module):
    embed_dim: Any = ...
    num_heads: Any = ...
    dropout: Any = ...
    head_dim: Any = ...
    scaling: Any = ...
    in_proj_weight: Any = ...
    in_proj_bias: Any = ...
    out_proj: Any = ...
    bias_k: Any = ...
    bias_v: Any = ...
    add_zero_attn: Any = ...
    def __init__(self, embed_dim: Any, num_heads: Any, dropout: float = ..., bias: bool = ..., add_bias_kv: bool = ..., add_zero_attn: bool = ...) -> None: ...
    def forward(self, query: Any, key: Any, value: Any, key_padding_mask: Optional[Any] = ..., incremental_state: Optional[Any] = ..., need_weights: bool = ..., static_kv: bool = ..., attn_mask: Optional[Any] = ...): ...

class PReLU(Module):
    num_parameters: Any = ...
    weight: Any = ...
    def __init__(self, num_parameters: int = ..., init: float = ...) -> None: ...
    def forward(self, input: Any): ...
    def extra_repr(self): ...

class Softsign(Module):
    def forward(self, input: Any): ...

class Tanhshrink(Module):
    def forward(self, input: Any): ...

class Softmin(Module):
    __constants__: Any = ...
    dim: Any = ...
    def __init__(self, dim: Optional[Any] = ...) -> None: ...
    def forward(self, input: Any): ...

class Softmax(Module):
    __constants__: Any = ...
    dim: Any = ...
    def __init__(self, dim: Optional[Any] = ...) -> None: ...
    def forward(self, input: Any): ...

class Softmax2d(Module):
    def forward(self, input: Any): ...

class LogSoftmax(Module):
    __constants__: Any = ...
    dim: Any = ...
    def __init__(self, dim: Optional[Any] = ...) -> None: ...
    def forward(self, input: Any): ...
