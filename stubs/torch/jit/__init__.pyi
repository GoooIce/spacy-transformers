# Stubs for torch.jit (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import torch.testing
from ..nn.modules.utils import _list_with_default, _pair, _quadruple, _single, _triple
from collections import namedtuple
from torch.nn import Module
from typing import Any, Optional

Future: Any

def scope(scope_name: Any) -> None: ...

DEFAULT_EXTRA_FILES_MAP: Any

def load(f: Any, map_location: Optional[Any] = ..., _extra_files: Any = ...): ...
def save(m: Any, f: Any, _extra_files: Any = ...) -> None: ...
def get_trace_graph(f: Any, args: Any = ..., kwargs: Optional[Any] = ..., _force_outplace: bool = ..., return_inputs: bool = ...): ...

class LegacyTracedModule(Module):
    inner: Any = ...
    def __init__(self, inner: Any, force_outplace: bool = ..., return_inputs: bool = ...) -> None: ...
    def forward(self, *args: Any): ...

def verify(model: Any, args: Any, loss_fn: Any = ..., devices: Optional[Any] = ...): ...
def indent(s: Any): ...

class TracingCheckError(Exception):
    message: str = ...
    def __init__(self, graph_diff_error: Any, tensor_compare_error: Any, extra_msg: Optional[Any] = ...) -> None: ...

class TracerWarning(Warning):
    @staticmethod
    def ignore_lib_warnings() -> None: ...

def trace(func: Any, example_inputs: Any, optimize: bool = ..., check_trace: bool = ..., check_inputs: Optional[Any] = ..., check_tolerance: float = ..., _force_outplace: bool = ..., _module_class: Optional[Any] = ...): ...

class CompilationUnit:
    def __init__(self, lang: Optional[Any] = ..., optimize: bool = ..., _frames_up: int = ...) -> None: ...
    def define(self, lang: Any, rcb: Optional[Any] = ..., _frames_up: int = ...) -> None: ...
    def __getattr__(self, attr: Any): ...

def whichmodule(obj: Any): ...
def script(obj: Any, optimize: bool = ..., _frames_up: int = ..., _rcb: Optional[Any] = ...): ...

ScriptMethodStub = namedtuple('ScriptMethodStub', ['resolution_callback', 'def_', 'original_method'])

def script_method(fn: Any, _rcb: Optional[Any] = ...): ...

class OrderedDictWrapper:
    module: Any = ...
    def __init__(self, module: Any) -> None: ...
    def keys(self): ...
    def values(self): ...
    def __delitem__(self, k: Any) -> None: ...
    def items(self) -> None: ...
    def __contains__(self, k: Any) -> None: ...
    def __getitem__(self, k: Any) -> None: ...
    def __setitem__(self, k: Any, v: Any) -> None: ...

class OrderedModuleDict(OrderedDictWrapper):
    def __init__(self, module: Any) -> None: ...
    def items(self): ...
    def __contains__(self, k: Any): ...
    def __setitem__(self, k: Any, v: Any) -> None: ...
    def __getitem__(self, k: Any): ...

class OrderedParameterDict(OrderedDictWrapper):
    def __init__(self, module: Any) -> None: ...
    def items(self): ...
    def __setitem__(self, k: Any, v: Any) -> None: ...
    def __contains__(self, k: Any): ...
    def __getitem__(self, k: Any): ...

class OrderedBufferDict(OrderedDictWrapper):
    def __init__(self, module: Any) -> None: ...
    def items(self): ...
    def __setitem__(self, k: Any, v: Any) -> None: ...
    def __contains__(self, k: Any): ...
    def __getitem__(self, k: Any): ...

class ScriptMeta(type):
    def __init__(cls, name: Any, bases: Any, attrs: Any) -> None: ...

class _CachedForward:
    def __get__(self, obj: Any, cls: Any): ...

class ScriptModule:
    def __init__(self, optimize: bool = ...) -> None: ...
    @property
    def graph(self): ...
    @property
    def code(self): ...
    def save(self, *args: Any, **kwargs: Any): ...
    def save_to_buffer(self, *args: Any, **kwargs: Any): ...
    def get_debug_state(self, *args: Any, **kwargs: Any): ...
    forward: Any = ...
    def __getattr__(self, attr: Any): ...
    def __setattr__(self, attr: Any, value: Any): ...
    def __dir__(self): ...
    def define(self, lang: Any) -> None: ...
    def copy(self): ...
    def graph_for(self, *args: Any, **kwargs: Any): ...

class WeakScriptModuleProxy(ScriptModule):
    def __init__(self, original: Any, stubs: Any) -> None: ...
    def __getattr__(self, attr: Any): ...
    def __setattr__(self, attr: Any, value: Any): ...

class ScriptModule(torch.nn.Module):
    def __init__(self, optimize: bool = ...) -> None: ...

class TracedModule(ScriptModule):
    training: Any = ...
    def __init__(self, orig: Any, id_set: Optional[Any] = ..., optimize: bool = ...) -> None: ...
    def forward(self, *args: Any, **kwargs: Any) -> None: ...
    def __setattr__(self, attr: Any, value: Any): ...

class TopLevelTracedModule(TracedModule):
    forward: Any = ...

class _ConstModuleList(ScriptModule):
    def __init__(self, modules: Any) -> None: ...
    def __getitem__(self, idx: Any): ...
    def __len__(self): ...
    def __iter__(self): ...
    def __dir__(self): ...

class _ConstSequential(_ConstModuleList):
    __constants__: Any = ...
    def __init__(self, mods: Any) -> None: ...

Error: Any

class _disable_tracing:
    state: Any = ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...

def annotate(the_type: Any, the_value: Any): ...

Attribute = namedtuple('Attribute', ['value', 'type'])
last_executed_optimized_graph: Any
Function: Any
