# Stubs for torch.autograd.function (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import torch.utils.hooks as _C
from typing import Any

class _ContextMethodMixin:
    to_save: Any = ...
    def save_for_backward(self, *tensors: Any) -> None: ...
    dirty_tensors: Any = ...
    def mark_dirty(self, *args: Any) -> None: ...
    def mark_shared_storage(self, *pairs: Any) -> None: ...
    non_differentiable: Any = ...
    def mark_non_differentiable(self, *args: Any) -> None: ...

class _HookMixin: ...

class BackwardCFunction(_C._FunctionBase, _ContextMethodMixin, _HookMixin):
    def apply(self, *args: Any): ...

class FunctionMeta(type):
    def __init__(cls, name: Any, bases: Any, attrs: Any) -> None: ...

class Function:
    __call__: Any = ...
    is_traceable: bool = ...
    @staticmethod
    def forward(ctx: Any, *args: Any, **kwargs: Any) -> None: ...
    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) -> None: ...

def once_differentiable(fn: Any): ...
def traceable(fn_cls: Any): ...

class InplaceFunction(Function):
    inplace: Any = ...
    def __init__(self, inplace: bool = ...) -> None: ...

class NestedIOFunction(Function):
    def backward(self, *gradients: Any): ...
    __call__: Any = ...
    def forward(self, *args: Any): ...
    to_save: Any = ...
    def save_for_backward(self, *args: Any) -> None: ...
    @property
    def saved_tensors(self): ...
    dirty_tensors: Any = ...
    def mark_dirty(self, *args: Any, **kwargs: Any) -> None: ...
    non_differentiable: Any = ...
    def mark_non_differentiable(self, *args: Any, **kwargs: Any) -> None: ...
    def forward_extended(self, *input: Any) -> None: ...
    def backward_extended(self, *grad_output: Any) -> None: ...
