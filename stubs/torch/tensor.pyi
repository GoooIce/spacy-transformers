# Stubs for torch.tensor (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import torch.utils.hooks
from typing import Any, Optional

class Tensor(torch._C._TensorBase):
    def __deepcopy__(self, memo: Any): ...
    def __reduce_ex__(self, proto: Any): ...
    def backward(self, gradient: Optional[Any] = ..., retain_graph: Optional[Any] = ..., create_graph: bool = ...) -> None: ...
    def register_hook(self, hook: Any): ...
    def reinforce(self, reward: Any): ...
    detach: Any = ...
    detach_: Any = ...
    retains_grad: bool = ...
    def retain_grad(self) -> None: ...
    def is_pinned(self): ...
    def is_shared(self): ...
    def share_memory_(self): ...
    def __reversed__(self): ...
    def norm(self, p: str = ..., dim: Optional[Any] = ..., keepdim: bool = ..., dtype: Optional[Any] = ...): ...
    def pstrf(self, upper: bool = ...): ...
    def potrf(self, upper: bool = ...): ...
    def potri(self, upper: bool = ...): ...
    def potrs(self, u: Any, upper: bool = ...): ...
    def gesv(self, A: Any): ...
    def trtrs(self, A: Any, upper: bool = ..., transpose: bool = ..., unitriangular: bool = ...): ...
    def btrifact(self, pivot: bool = ...): ...
    def btrifact_with_info(self, pivot: bool = ...): ...
    def btrisolve(self, LU_data: Any, LU_pivots: Any): ...
    def lu(self, pivot: bool = ..., get_infos: bool = ...): ...
    def stft(self, n_fft: Any, hop_length: Optional[Any] = ..., win_length: Optional[Any] = ..., window: Optional[Any] = ..., center: bool = ..., pad_mode: str = ..., normalized: bool = ..., onesided: bool = ...): ...
    def resize(self, *sizes: Any): ...
    def resize_as(self, tensor: Any): ...
    def split(self, split_size: Any, dim: int = ...): ...
    def unique(self, sorted: bool = ..., return_inverse: bool = ..., return_counts: bool = ..., dim: Optional[Any] = ...): ...
    def unique_consecutive(self, return_inverse: bool = ..., return_counts: bool = ..., dim: Optional[Any] = ...): ...
    def __rsub__(self, other: Any): ...
    def __rdiv__(self, other: Any): ...
    __rtruediv__: Any = ...
    __itruediv__: Any = ...
    __pow__: Any = ...
    def __format__(self, format_spec: Any): ...
    def __ipow__(self, other: Any) -> None: ...
    def __rpow__(self, other: Any): ...
    def __floordiv__(self, other: Any): ...
    def __rfloordiv__(self, other: Any): ...
    __neg__: Any = ...
    __eq__: Any = ...
    __ne__: Any = ...
    __lt__: Any = ...
    __le__: Any = ...
    __gt__: Any = ...
    __ge__: Any = ...
    __abs__: Any = ...
    def __len__(self): ...
    def __iter__(self): ...
    def __hash__(self): ...
    def __dir__(self): ...
    __array_priority__: int = ...
    def __array__(self, dtype: Optional[Any] = ...): ...
    def __array_wrap__(self, array: Any): ...
    def __contains__(self, element: Any): ...
    @property
    def __cuda_array_interface__(self): ...
    __module__: str = ...
