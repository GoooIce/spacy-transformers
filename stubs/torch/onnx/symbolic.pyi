# Stubs for torch.onnx.symbolic (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from typing import Any, Optional

def parse_args(*arg_descriptors: Any): ...
def unused(g: Any): ...
def reshape(g: Any, self: Any, shape: Any): ...
def reshape_as(g: Any, self: Any, other: Any): ...
def add(g: Any, self: Any, other: Any, alpha: Optional[Any] = ...): ...
def sub(g: Any, self: Any, other: Any, alpha: Optional[Any] = ...): ...
def rsub(g: Any, self: Any, other: Any, alpha: Optional[Any] = ...): ...
def mul(g: Any, self: Any, other: Any): ...
def div(g: Any, self: Any, other: Any): ...
def reciprocal(g: Any, self: Any): ...
def cat(g: Any, tensor_list: Any, dim: Any): ...
def stack(g: Any, tensor_list: Any, dim: Any): ...
def mm(g: Any, self: Any, other: Any): ...
def bmm(g: Any, self: Any, other: Any): ...
def matmul(g: Any, self: Any, other: Any): ...
def addmm(g: Any, self: Any, mat1: Any, mat2: Any, beta: Any, alpha: Any): ...
def neg(g: Any, self: Any): ...
def sqrt(g: Any, self: Any): ...
def tanh(g: Any, self: Any): ...
def sin(g: Any, self: Any): ...
def cos(g: Any, self: Any): ...
def tan(g: Any, self: Any): ...
def asin(g: Any, self: Any): ...
def acos(g: Any, self: Any): ...
def atan(g: Any, self: Any): ...
def sigmoid(g: Any, self: Any): ...

mean: Any
sum: Any
prod: Any

def cumsum(g: Any, input: Any, dim: Any): ...
def t(g: Any, self: Any): ...
def expand(g: Any, self: Any, size: Any, implicit: Any): ...
def expand_as(g: Any, self: Any, other: Any): ...
def embedding(g: Any, weight: Any, indices: Any, padding_idx: Any, scale_grad_by_freq: Any, sparse: Any): ...
def embedding_bag(g: Any, embedding_matrix: Any, indices: Any, offsets: Any, scale_grad_by_freq: Any, mode: Any, sparse: Any, per_sample_weights: Any): ...
def size(g: Any, self: Any, dim: Any): ...
def transpose(g: Any, self: Any, dim0: Any, dim1: Any): ...
def permute(g: Any, self: Any, dims: Any): ...
def view(g: Any, self: Any, size: Any): ...
def prim_ConstantSplit(g: Any, self: Any, split_size: Any, dim: Any): ...
def prim_ConstantChunk(g: Any, self: Any, chunks: Any, dim: Any): ...
def split(g: Any, self: Any, split_size: Any, dim: Any): ...
def split_with_sizes(g: Any, self: Any, split_sizes: Any, dim: Any): ...
def select(g: Any, self: Any, dim: Any, index: Any): ...
def squeeze(g: Any, self: Any, dim: Optional[Any] = ...): ...
def prelu(g: Any, self: Any, weight: Any): ...
def relu(g: Any, input: Any): ...
def ceil(g: Any, input: Any): ...
def floor(g: Any, input: Any): ...
def threshold(g: Any, self: Any, threshold: Any, value: Any): ...
def leaky_relu(g: Any, input: Any, negative_slope: Any, inplace: bool = ...): ...
def glu(g: Any, input: Any, dim: Any): ...
def softmax(g: Any, input: Any, dim: Any, dtype: Optional[Any] = ...): ...
def softplus(g: Any, self: Any, beta: Any, threshold: Any): ...
def get_pool_ceil_padding(input: Any, kernel_size: Any, stride: Any, padding: Any): ...

max_pool1d: Any
max_pool2d: Any
max_pool3d: Any
max_pool1d_with_indices: Any
max_pool2d_with_indices: Any
max_pool3d_with_indices: Any
avg_pool1d: Any
avg_pool2d: Any
avg_pool3d: Any
adaptive_avg_pool1d: Any
adaptive_avg_pool2d: Any
adaptive_avg_pool3d: Any
adaptive_max_pool1d: Any
adaptive_max_pool2d: Any
adaptive_max_pool3d: Any

def constant_pad_nd(g: Any, input: Any, padding: Any, value: Any): ...
def reflection_pad(g: Any, input: Any, padding: Any): ...
def replication_pad(g: Any, input: Any, padding: Any): ...
reflection_pad1d = reflection_pad
reflection_pad2d = reflection_pad
reflection_pad3d = reflection_pad
replication_pad1d = replication_pad
replication_pad2d = replication_pad
replication_pad3d = replication_pad

def upsample_nearest2d(g: Any, input: Any, output_size: Any): ...
def upsample_bilinear2d(g: Any, input: Any, output_size: Any, align_corners: Any): ...
def wrap_logical_op_with_cast_to_uint8(func: Any): ...
def wrap_logical_op_with_negation(func: Any): ...
def eq(g: Any, self: Any, other: Any): ...
def ne(g: Any, self: Any, other: Any): ...
def gt(g: Any, input: Any, other: Any): ...
def gt_impl(g: Any, input: Any, other: Any): ...
def lt(g: Any, input: Any, other: Any): ...
def lt_impl(g: Any, input: Any, other: Any): ...
def ge(g: Any, input: Any, other: Any): ...
def le(g: Any, input: Any, other: Any): ...
def where(g: Any, condition: Any, self: Any, other: Any): ...
def log_softmax(g: Any, input: Any, dim: Optional[Any] = ..., dtype: Optional[Any] = ...): ...
def batch_norm(g: Any, input: Any, weight: Any, bias: Any, running_mean: Any, running_var: Any, training: Any, momentum: Any, eps: Any, cudnn_enabled: Any): ...
def instance_norm(g: Any, input: Any, weight: Any, bias: Any, running_mean: Any, running_var: Any, use_input_stats: Any, momentum: Any, eps: Any, cudnn_enabled: Any): ...
def unfold(g: Any, input: Any, dimension: Any, size: Any, step: Any): ...
def elu(g: Any, input: Any, alpha: Any, scale: Any, input_scale: Any): ...
def selu(g: Any, input: Any): ...
def index_select(g: Any, self: Any, dim: Any, index: Any): ...
def index_put(g: Any, self: Any, indices_list_value: Any, values: Any, accumulate: Any): ...
def type_as(g: Any, self: Any, other: Any): ...
def layer_norm(g: Any, self: Any, normalized_shape: Any, weight: Any, bias: Any, eps: Any, cudnn_enable: Any): ...
def clone(g: Any, input: Any): ...
def abs(g: Any, self: Any): ...
def log(g: Any, self: Any): ...
def pow(g: Any, self: Any, exponent: Any): ...
def clamp(g: Any, self: Any, min: Any, max: Any): ...
def clamp_min(g: Any, self: Any, min: Any): ...
def clamp_max(g: Any, self: Any, max: Any): ...
def max(g: Any, self: Any, dim_or_y: Optional[Any] = ..., keepdim: Optional[Any] = ...): ...
def min(g: Any, self: Any, dim_or_y: Optional[Any] = ..., keepdim: Optional[Any] = ...): ...
def exp(g: Any, self: Any): ...
def dropout(g: Any, input: Any, p: Any, train: Any): ...

feature_dropout: Any
alpha_dropout: Any
feature_alpha_dropout: Any
dropout_ = dropout
feature_dropout_ = feature_dropout
alpha_dropout_ = alpha_dropout
feature_alpha_dropout_ = feature_alpha_dropout

def norm(g: Any, self: Any, p: Any, dim: Any, keepdim: Any): ...
def conv_tbc(g: Any, input: Any, weight: Any, bias: Any, pad: Any): ...

cast_pytorch_to_onnx: Any
scalar_name_to_pytorch: Any
scalar_type_to_pytorch_type: Any
name: Any
scalar_type_to_onnx: Any

def zeros(g: Any, sizes: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def zeros_like(g: Any, input: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def ones(g: Any, sizes: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def ones_like(g: Any, input: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def full(g: Any, sizes: Any, value: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def full_like(g: Any, input: Any, fill_value: Any, dtype: Any, layout: Any, device: Any, pin_memory: bool = ...): ...
def slice(g: Any, self: Any, dim: Any, start: Any, end: Any, step: Any): ...
def hardtanh(g: Any, self: Any, min_val: Any, max_val: Any): ...
def alias(g: Any, self: Any): ...
def unsqueeze(g: Any, self: Any, dim: Any): ...
def topk(g: Any, self: Any, k: Any, dim: Any, largest: Any, sorted: Any, out: Optional[Any] = ...): ...
def to(g: Any, self: Any, *args: Any): ...
def repeat(g: Any, self: Any, repeats: Any): ...
def pixel_shuffle(g: Any, self: Any, upscale_factor: Any): ...
def group_norm(g: Any, input: Any, num_groups: Any, weight: Any, bias: Any, eps: Any, cudnn_enabled: Any): ...
def lstm(g: Any, *args: Any): ...

gru: Any
rnn_tanh: Any
rnn_relu: Any

def detach(g: Any, input: Any): ...
def contiguous(g: Any, input: Any): ...
def randn(g: Any, *shapes: Any): ...
def rrelu(g: Any, input: Any, lower: Any, upper: Any, training: Any, generator: Any): ...
def log_sigmoid(g: Any, input: Any): ...
def erf(g: Any, input: Any): ...
def flatten(g: Any, input: Any, start_dim: Any, end_dim: Any): ...
def nonzero(g: Any, input: Any): ...
def isnan(g: Any, input: Any): ...
def narrow(g: Any, input: Any, dim: Any, start: Any, length: Any): ...
def argmax(g: Any, input: Any, dim: Any, keepdim: Any): ...
def argmin(g: Any, input: Any, dim: Any, keepdim: Any): ...
