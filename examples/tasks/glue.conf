[defaults]

# Model architecture for textcat
textcat_arch = softmax_pooler_output

# The maximum total input sequence length after tokenization.
max_seq_length = 128

# Dropout rate
dropout = 0.1

# Number of examples per training batch
batch_size = 32

# Number of examples per eval batch
eval_batch_size = 32

# The initial learning rate for Adam
learning_rate = 2e-5

# Weight decay (aka L2 penalty)
weight_decay = 0.0

# Epsilon for the Adam optimizer
adam_epsilon = 1e-8

# Max gradient norm (aka gradient clipping)
max_grad_norm = 1.0

# Number of training epochs to perform
num_train_epochs = 3

# Total number of training iterations to perform. If set, overrides num_train_epochs
max_steps = -1

# Linear warmup of learning rate over warmup_steps
warmup_steps = 0

# Whether to use the learning rate schedule (or keep LR fixed)
use_learn_rate_schedule = 0

# Whether to use stochastic weight averaging
use_swa = 1

# Set random seed
seed = 0

# How often (by steps) to evaluate
eval_every = 1000
